@article{das2025free,
  title={A Free Probabilistic Framework for Analyzing the Transformer-based Language Models},
  author={Das, Swagatam},
  journal={Statistics and Probability Letters},
  volume = {226},
  publisher = {Elsevier},
  year={2025},
  abstract={ We present a formal operator-theoretic framework for analyzing Transformer-based language models using free probability theory. By modeling token embeddings and attention mechanisms as self-adjoint operators in a tracial $W^∗$-probability space, we reinterpret attention as non-commutative convolution and describe representation propagation via free additive convolution. This leads to a spectral dynamic system interpretation of deep Transformers. We derive entropy-based generalization bounds under freeness assumptions and provide insight into positional encoding, spectral evolution, and representational complexity. This work offers a principled, though theoretical, perspective on structural dynamics in large language models.},
  keywords={Transformers, Free Probability, Spectral Theory, Non-Commutative Random Variables, Language Model},
  jctype={journal},
}

@inproceedings{basu2023robust,
  title={Robust and Automatic Data Clustering: Dirichlet Process meets Median-of-Means},
  author={Basu, Supratik and Choudhury, Jyotishka Ray and Paul, Debolina and Das, Swagatam},
  booktitle={International Joint Conference on Artificial Intelligence (IJCAI)},
  year={2025},
  keywords={Clṭustering},
  abstract={Clustering stands as one of the most prominent challenges in unsupervised machine learning. Among centroid-based methods, the classic k-means algorithm, based on Lloyd’s heuristic, is widely used. Nonetheless, it is a well-known fact that k-means and its variants face several challenges, including heavy reliance on initial cluster centroids, susceptibility to converging into local minima of the objective function, and sensitivity to outliers and noise in the data. When data contains noise or outliers, the Median-of-Means (MoM) estimator offers a robust alternative for stabilizing centroid-based methods. On a different note, another limitation in many commonly used clustering methods is the need to specify the number of clusters beforehand. Model-based approaches, such as Bayesian nonparametric models, address this issue by incorporating infinite mixture models, which eliminate the requirement for predefined cluster counts. Motivated by these facts, in this article, we propose an efficient and automatic clustering technique by integrating the strengths of model-based and centroid-based methodologies. Our method mitigates the effect of noise on the quality of clustering; while at the same time, estimates the number of clusters. Statistical guarantees on an upper bound of clustering error, and rigorous assessment through simulated and real datasets, suggest the advantages of our proposed method over existing state-of-the-art clustering algorithms.},
  jctype= {conference},
}

@article{chakrabarty2025gromov
  author = {Chakrabarty, Anish and Subhra Mullick, Sankha and Das, Swagatam},
  title = {Locally Robust Alignment Between Distinct Spaces},
  journal = {Stat},
  volume = {14},
  number = {3},
  pages = {e70093},
  keywords = {Gromov-Wasserstein distance, Optimal transport, Robustness},
  doi = {https://doi.org/10.1002/sta4.70093},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sta4.70093},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sta4.70093},
  note = {e70093 sta4.70093},
  abstract = {The Gromov-Wasserstein (GW) distance serves as a measure of discrepancy between two distributions that are supported on distinct ambient spaces. Emerging as the optimal expected departure from isometry, it keeps finding continual usage in tasks such as object matching and network analysis. However, its merit is often undermined by its susceptibility to outliers present in data. Thus, in this work, we focus on studying the statistical properties of _Locally Robust GW_, a newly introduced robust formulation that preserves topological properties from the original distance. We show that it recovers unperturbed GW values under contamination, making it a suitable proxy loss for several machine learning tasks. Based on the same, we also develop a robust transform sampling framework with supporting concentration bounds.},
  year = {2025},
  jctype={journal},
}


@article{roy2025wasserstein,
  author={Roy, Srinjoy and Saha, Subhajit and Das, Swagatam},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence (TETCI)}, 
  title={From Wasserstein to Maximum Mean Discrepancy Barycenters: A Novel Framework for Uncertainty Propagation in Model-Free Reinforcement Learning}, 
  year={2025},
  volume={},
  number={},
  pages={1-13},
  abstract={Uncertainty characterization via posteriors followed by Bayesian updates is an acclaimed way to aid the exploration of model-free Reinforcement Learning (RL) algorithms. Motivated by the Bayesian RL literature, we build Maximum Mean Discrepancy Q-Learning (MMD-QL) to enhance the well-known Wasserstein Q-Learning (WQL) for more accurately estimating the posteriors and, as a result, accomplish even better exploration. MMD-QL leverages the Maximum Mean Discrepancy (MMD) barycenter, as for many positive semi-definite kernels that induce smooth function classes, MMD offers a more precise notion of distance between distributions than the Wasserstein distance. We analytically prove that a slightly modified version of MMD-QL is Probably Approximately Correct in Markov Decision Processes (PAC-MDP) under the average loss metric. Thorough experiments on several tabular domains illustrate that MMD-QL mainly surpasses the performance of WQL and other algorithms. Unlike many provably efficient RL algorithms, the framework of MMD-QL can extend to function approximation. Specifically, we incorporate Bootstrapped DQN-style deep networks, giving rise to Maximum Mean Discrepancy Q-Network (MMD-QN). Our experiments on a few Atari games reveal that MMD-QN delivers better than the deep equivalent of WQL, emphasizing the efficacy of MMD barycenters for propagating uncertainty in environments with large state-action space size.},
  keywords={Uncertainty, Q-learning, Kernel, Bayes methods, Approximation algorithms, Mathematical models, Markov decision processes, Games, Complexity theory, Trajectory, Reinforcement learning, Uncertainty propagation, maximum mean discrepancy},
  doi={10.1109/TETCI.2025.3593841},
  ISSN={2471-285X},
  month={},
  jctype={journal},
}

@article{ansari2025sampling,
  title = {Sampling-tailored two-pronged network for long-tailed class imbalance learning},
  journal = {Engineering Applications of Artificial Intelligence (EAAI)},
  volume = {159},
  pages = {111466},
  year = {2025},
  issn = {0952-1976},
  doi = {https://doi.org/10.1016/j.engappai.2025.111466},
  url = {https://www.sciencedirect.com/science/article/pii/S095219762501468X},
  author = {Ansari, Faizanuddin and Panigrahi, Abhranta and Das, Swagatam},
  keywords = {Long-tailed distribution, Class imbalance, Visual recognition, Classification, Oversampling, Augmentation},
  abstract = {A long-tail class imbalanced learning problem is a scenario where the rare or minority classes, representing infrequent events or categories, make up the long tail of the class distribution and have disproportionately few examples compared to the dominant classes. The resulting imbalance makes it challenging to train models effectively for these underrepresented classes. We introduce a comprehensive solution - **STTP**-Net: **S**ampling-**T**ailored **T**wo-**P**ronged Network for long-tail class-imbalanced learning, which aims to address this issue holistically. The study thoroughly examines mixed sample data augmentation techniques in conjunction with various sampling strategies to identify the most effective approaches for handling long-tail imbalance. Based on this analysis, a hybrid mixup strategy tailored explicitly for data augmentation in long-tail imbalanced settings is proposed. The core of the proposed approach comprises a two-pronged network consisting of two classification heads designed to handle long-tail imbalanced datasets. One head specializes in learning the head and median classes in this design. In contrast, the other head becomes an expert in tail classes, striking a balance between accurate prediction of tail classes without compromising accuracy for the head classes. Additionally, we address the label distribution shifts in long-tail imbalance by introducing an Effective Balanced Softmax (EBS) function. The presented method achieves state-of-the-art performance in several benchmark categories for long-tail visual recognition datasets, surpassing the most prominent and pertinent end-to-end and dual-branch approaches.},
  jctype= {journal},
}

@article{chakrabarty2025information,
  title="Information preservation with wasserstein autoencoders: generation consistency and adversarial robustness",
  author="Chakrabarty, Anish and Basu, Arkaprabha and Das, Swagatam",
  journal="Statistics and Computing",
  volume="35",
  abstract="Amongst the numerous variants Variational Autoencoder (VAE) has inspired, the Wasserstein Autoencoder (WAE) stands out due to its heightened generative quality and intriguing theoretical properties. WAEs consist of an encoding and a decoding network— forming a bottleneck— with the prime objective of generating new samples resembling the ones it was catered to. In the process, they aim to achieve a target latent representation of the encoded data. Our work offers a comprehensive theoretical understanding of the machinery behind WAEs. From a statistical viewpoint, we pose the problem as concurrent density estimation tasks based on neural network-induced transformations. This allows us to establish deterministic upper bounds on the realized errors WAEs commit, supported by simulations on real and synthetic data sets. We also analyze the propagation of these stochastic errors in the presence of adversaries. As a result, both the large sample properties of the reconstructed distribution and the resilience of WAE models are explored.",
  number="5",
  pages="1--27",
  year="2025",
  publisher="Springer",
  keywords="Deep generative models, Wasserstein autoencoder, Maximum Mean Discrepancy, Minimum distance estimation, Robustness",
  jctype="journal",
  doi={10.1007/s11222-025-10657-z},
}

@article{mondal2025force,
  author={Mondal, Priyobrata and Ansari, Faizanuddin and Das, Swagatam and Shamsolmoali, Pourya},
  journal={IEEE  International Joint Conference on Neural Networks (IJCNN)}, 
  title={Force of Attraction-Based Distribution Calibration for Enhancing Minority Class Representation}, 
  year={2025},
  volume={},
  number={},
  abstract={Imbalanced image datasets pose significant challenges for developing robust classifiers, particularly when certain classes are heavily underrepresented. To tackle this issue, we propose Density-Driven Attraction (DDA) Oversampling, a novel technique designed to improve class representation in the latent space. Our approach begins by projecting images into disentangled latent representations, ensuring clear separation between classes and precise identification of subclasses. At the core of this method is the Density-Driven Attraction Force (DDAF), a mechanism inspired by gravitational forces. DDAF quantifies the attraction between components of well-represented and underrepresented classes, adjusting the attraction based on the density of each component. This process recalibrates the distributions of underrepresented classes by leveraging their strongest attractions, effectively simulating the natural principles of mass attraction. Extensive classification experiments on six multiclass imbalanced datasets demonstrate that DDA Oversampling outperforms existing state-of-the-art methods, resulting in more accurate and balanced class distributions.},
  keywords={ Class Imbalance, Neural Networks, Image Classification, Density-Driven Attraction },
  jctype={conference},
}

@article{bose2025graph,
  author={Bose, Kushal and Banerjee, Saptarshi and Das, Swagatam},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Can Graph Neural Networks Tackle Heterophily? Yes, With a Label-Guided Graph Rewiring Approach!}, 
  year={2025},
  volume={},
  number={},
  pages={1-15},
  abstract={Graph neural networks (GNNs) witness impressive performances on homophilic graphs characterized by a higher number of edges connecting nodes of similar class labels. A decline in the performance of GNNs can be experienced when applied to heterophilic graphs where most of the edges connect nodes with different class labels. This study presents a novel and versatile preprocessing framework comprising three fundamental stages. This framework can be seamlessly integrated with various GNN architectures to address heterophily within graphs effectively. In the initial stage, we predict class probabilities for nodes through a dense network. It is widely acknowledged that conventional feature-based similarity measures, such as cosine similarity, might not always accurately capture the correspondence between node pairs. Moving to the second stage, we introduce a reweighting strategy guided by class embeddings generated from autoencoders to counter this limitation. In the final stage, we utilize the reweighted similarity coefficients in a two-stage graph rewiring process. This process involves node deletion and subsequent insertion to generate a more homophily-oriented neighborhood. We reuse class embeddings by fusing them with the original node features to enrich the node features with class-level information. The updated node features and the rewired graph structure are ultimately fed into the GNN model. This facilitates effective message passing (MP) across neighborhoods. We extensively evaluate our approach on various standard graph datasets encompassing homophilic and heterophilic characteristics. Across these datasets, our framework consistently improves the performance of the established baseline methods.},
  keywords={Autoencoders;Training;Topology;Graph neural networks;Standards;Network topology;Message passing;Learning systems;Histograms;Hands;Convolution;graph;heterophily;message passing (MP);rewiring},
  doi={10.1109/TNNLS.2025.3565108},
  ISSN={2162-2388},
  month={},
  jctype={journal},
}



@article{ansari2025goldilocks,
  author={Ansari, Faizanuddin and Panigrahi, Abhranta and Das, Swagatam},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence}, 
  title={The Goldilocks Principle: Achieving Just Right Boundary Fidelity for Long-Tailed Classification}, 
  year={2025},
  volume={},
  number={},
  pages={1-15},
  abstract={This study addresses the challenges of learning from long-tailed class imbalances in deep neural networks, particularly for image recognition. Long-tailed class imbalances occur when a dataset's class distribution is highly skewed, with a few head classes containing many instances and numerous tail classes having fewer instances. This imbalance becomes problematic when traditional classification methods, especially deep learning models, prioritize accuracy in the more frequent classes, neglecting the less common ones. Furthermore, these methods struggle to maintain consistent boundary fidelity—decision boundaries that are sharp enough to distinguish classes yet smooth enough to generalize well. Hard boundaries, often caused by overfitting tail classes, amplify intra-class variations, while overly soft boundaries blur distinctions between classes, reducing classification accuracy. We propose a dual-branch network with a shared feature extractor to overcome these challenges. This network uses instance and median samplers for head and medium classes and a reverse sampler for tail classes. Additionally, we implement a specialized loss function as a feature regularizer to reduce the model's sensitivity to irrelevant intra-class variations during classification. This loss function dynamically modulates feature representation alignment, promoting cohesive intra-class structures and clear inter-class separations. To achieve this, our framework incorporates two key components: Dual-Branch Sampler-Guided Mixup (DBSGM) and Adaptive Class-Aware Feature Regularizer (ACFR), which work together to balance class representation and refine decision boundaries. Integrating DBSGM and ACFR during training helps shape decision boundaries that align with class semantics. To ensure class boundaries are appropriately defined, we propose the temperature-adaptive supervised contrastive loss (TASCL) within the ACFR module, achieving the right balance between smoothness and sharpness. Our single-stage, end-to-end framework demonstrates significant performance improvements, offering a promising solution to the challenges of long-tailed class imbalances in deep learning.},
  keywords={Heavily-tailed distribution;Training;Image recognition;Feature extraction;Adaptation models;Robustness;Data augmentation;Class imbalance;long-tailed classification;feature regularization;mixup},
  doi={10.1109/TETCI.2025.3551950},
  ISSN={2471-285X},
  jctype={journal},
}

@article{CHOWDHURY2025103012,
title = {Deep multi-view clustering: A comprehensive survey of the contemporary techniques},
journal = {Information Fusion},
volume = {119},
pages = {103012},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.103012},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525000855},
author = {Chowdhury, Anal Roy and Gupta, Avisek and Das, Swagatam},
keywords = {Multi-view clustering, Deep learning, Autoencoders, Subspace clustering, Metric learning},
abstract = {Data can be represented by multiple sets of features, where each semantically coherent set of features is called a view. For example, an image can be represented by multiple sets of features that measure textures, shapes, edge features, etc. Collecting multiple views of data is generally easier than annotating it with the help of experts. Thus, the unsupervised exploration of data in consultation with all collected views is essential to identify naturally occurring clusters of data instances. In deep multi-view clustering, deep neural networks are used to obtain non-linear latent representations of data instances that agree with the multiple views, using which clusters of data instances are identified. A wide variety of such deep multi-view clustering approaches exist, which we systematically study and categorize into a novel taxonomy that provides structure to the existing literature and can also guide future researchers. We provide a pedagogical discussion on preliminary concepts to help understand topics relevant to the studied deep clustering methods. Various multi-view problems that are being studied are summarized, and future research scopes have been noted.},
jctype={journal},
}



@inproceedings{ghosh2025enhancing,
    author = "Ghosh, Susmita and Das, Swagatam",
    title = "Enhancing Medical Image Analysis with MA-DTNet: A Dual Task Network Guided by Morphological Attention",
    booktitle = "International Conference on Pattern Recognition (ICPR)",
    abstract = "Accurate breast tumor segmentation and malignancy detection are crucial for early cancer diagnosis. In this context, we propose a novel lightweight multi-task learning framework, MA-DTNet, designed to perform both tasks simultaneously in an encoder-shared scenario. This approach leverages shared representations and contextual information, enabling mutual enhancement of the tasks. Unlike existing methods that require a large number of trainable parameters, MA-DTNet integrates a Spatial Morphological Attention (SMA) module alongside a Channel Attention (CA) mechanism to strategically enhance crucial morphological features and emphasize informative channels within the extracted representations. The SMA mechanism combines traditional morphological operations with trainable, adaptive structuring elements, effectively highlighting critical morphological attributes of regions of interest (ROIs) of various shapes and sizes within medical images. This targeted emphasis on morphological features translates to improved performance in both segmentation and classification tasks. Notably, MA-DTNet demonstrates superior performance compared to state-of-the-art multi-task learning (MTL) and single-task models on two publicly available breast ultrasound datasets. Specifically, on the UDIAT dataset, our approach achieves a 3.28 and 1.05 enhancement in dice score (segmentation) and F1 score (classification), respectively. Similarly, for the BUSI dataset, MA-DTNet exhibits a 1.62 and 4.96 improvement in dice score and accuracy, respectively. Significantly, MA-DTNet achieves these performance gains with significantly fewer trainable parameters than existing methods, underscoring its efficiency and potential for real-world applications. The method’s generalization ability is further tested on two additional multi-task learning tasks: segmenting and classifying glands in histology images and segmenting and classifying skin lesions in dermoscopic images.",
    pages = "283--300",
    year = "2025",
    organization = "Springer",
    jctype = "conference",
    keywords="CV",
    doi = "10.1007/978-3-031-78198-8_19",
    isbn="978-3-031-78198-8",
}

@inproceedings{basu2025improved,
    author = "Basu, Arkaprabha and Raha, Sourav and Gupta, Avisek and Das, Swagatam",
    title = "Improved Alzheimer’s Disease Detection with Dynamic Attention Guided Multi-modal Fusion",
    booktitle = "International Conference on Pattern Recognition (ICPR)",
    pages = "432--446",
    year = "2025",
    abstract = "The early detection of neurodegenerative disorders such as Alzheimer’s disease is crucial to providing effective healthcare for management and recovery. We address the task of ternary classification of healthy, mild cognitive impairment, and Alzheimer’s disease categories from multiple data modalities of 3D MRIs, patient electronic health records, and genetic information. For this task, we propose a Dynamic Attention Guided Multi-modal Fusion (DAGMF) approach, broadly consisting of three deep network components. The first component independently performs feature extraction for all modalities and refines them using novel Per-Modality Attention blocks. Thereafter, the obtained modality representations are provided to a proposed Dynamic Attention Multi-modal Solver block, which models the dynamics of attention across learning iterations by a Neural Ordinary Differential Equation (NODE) solver to generate modality attention. The modality representations and attention are finally provided to a novel Attention-induced Multi-modal Fusion block, which uses the attention to perform late-fusion of the multiple modality representations by a second NODE solver, which models dynamics of the various modalities across learning iterations. Empirical studies on multi-modal datasets constructed from the ADNI collection show that the proposed DAGMF method provides better classification performance than state-of-the-art multi-modal deep learning approaches.",
    organization = "Springer",
    jctype = "conference",
    keywords="Application, Attention",
    featured = "true",
    doi="10.1007/978-3-031-78195-7_29",
}

@article{kumar2024enhancing,
  title={Enhancing Contrastive Clustering with Negative Pair-guided Regularization},
  author={Kumar, Abhishek and Chakrabarty, Anish and Mullick, Sankha Subhra and Das, Swagatam},
  journal={Transactions on Machine Learning Research},
  abstract={Contrastive Learning (CL) aims to create effective embeddings for input data by minimizing the distance between positive pairs, i.e., different augmentations or views of the same sample. To avoid degeneracy, CL also employs auxiliary loss to maximize the discrepancy between negative pairs formed with views of distinct samples. As a self-supervised learning strategy, CL inherently attempts to cluster input data into natural groups. However, the often improper trade-off between the attractive and repulsive forces, respectively induced by positive and negative pairs, can lead to deformed clustering, particularly when the number of clusters $k$ is unknown. To address this, we propose NRCC, a CL-based deep clustering framework that generates cluster-friendly embeddings. NRCC repurposes Stochastic Gradient Hamiltonian Monte Carlo sampling as an approximately invariant data augmentation, to curate hard negative pairs that judiciously enhance and balance the two adversarial forces through a regularizer. By preserving the cluster structure in the CL embedding, NRCC retains local density landscapes in lower dimensions through neighborhood-conserving projections. This enables the application of mode-seeking clustering algorithms, typically hindered by high-dimensional CL feature spaces, to achieve exceptional accuracy without needing a predetermined $k$. NRCC's superiority is demonstrated across various datasets with different scales and cluster structures, outperforming $20$ state-of-the-art methods.},
  year={2024},
  jctype={journal},
  featured = {true},
  url={https://openreview.net/forum?id=y4VYzqQ4},
  month={12},
  keywords={Constrative Learning, CLustering, Regularization},
}

@article{GHOSH2024109047,
title = {Multi-scale morphology-aided deep medical image segmentation},
journal = {Engineering Applications of Artificial Intelligence},
volume = {137},
pages = {109047},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109047},
author = {Ghosh, Susmita and Das, Swagatam},
keywords = {Medical image segmentation, Mathematical morphology, Multiscale trainable morphological modules, Deep convolutional networks, UNet},
abstract = {Medical image segmentation serves as a critical tool for healthcare professionals, enabling the precise extraction of Regions of Interest (ROIs) from clinical images at the pixel level. The evolution of computer vision and machine learning algorithms has streamlined this labor-intensive segmentation process, which traditionally necessitates domain expertise. The intrinsic challenges posed by clinical images — such as irregular shapes, varying sizes, low contrast, and intricate details within specific areas, contribute to the intricacy of the task. In response to these complexities, we propose a solution involving three modules grounded in morphological operations: the Multi-scale Morphological Closing Module, the Multi-scale Morphological Opening Module, and the Multi-scale Morphological Gradient Module . In contrast to conventional morphological operations, our approach involves learning structuring elements through a training process, enabling effective adaptation to the irregular shapes of ROIs. To cater to the diverse range of ROI sizes in clinical images, we introduce the concept of dilation rates within the structural elements of morphological operations. Our proposal extends to Morph-UNet, a lightweight framework for medical image segmentation. This architecture integrates proposed modules with UNet, presenting a tight-coupled synergy between deep neural networks and multi-scale morphological operations. Efficacy is substantiated across diverse medical imaging datasets, spanning modalities, conditions, and ROI proportions. Extensive experimentation validated through widely recognized segmentation metrics underscores our model’s superiority compared to fifteen state-of-the-art segmentation methods and baseline models.},
jctype={journal},
}

@inproceedings{ansari2024algorithmic,
author="Ansari, Faizanuddin and Chakraborti, Tapabrata and Das, Swagatam",
title="Algorithmic Fairness in Lesion Classification by Mitigating Class Imbalance and Skin Tone Bias",
booktitle="Medical Image Computing and Computer Assisted Intervention (MICCAI)",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="373--382",
abstract="Deep learning models have shown considerable promise in the classification of skin lesions. However, a notable challenge arises from their inherent bias towards dominant skin tones and the issue of imbalanced class representation. This study introduces a novel data augmentation technique designed to address these limitations. Our approach harnesses contextual information from the prevalent class to synthesize various samples representing minority classes. Using a mixup-based algorithm guided by an adaptive sampler, our method effectively tackles bias and class imbalance issues. The adaptive sampler dynamically adjusts sampling probabilities based on the network's meta-set performance, enhancing overall accuracy. Our research demonstrates the efficacy of this approach in mitigating skin tone bias and achieving robust lesion classification across a spectrum of diverse skin colors from two distinct benchmark datasets, offering promising implications for improving dermatological diagnostic systems.",
isbn="978-3-031-72378-0",
jctype="conference",
featured="true",
keywords="Algorithmic Fairness, Class Imbalance",
}

@article{mondal2024cco,
  author={Mondal, Priyobrata and Ansari, Faizanuddin and Das, Swagatam},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence}, 
  title={CCO: A Cluster Core-Based Oversampling Technique for Improved Class-Imbalanced Learning}, 
  year={2025},
  volume={9},
  number={2},
  pages={1153-1165},
  abstract={Supervised classification problems from the real world typically face a challenge characterized by the scarcity of samples in one or more target classes compared to the rest of the majority classes. In response to such class imbalance, we propose an oversampling technique based on clustering, aiming to populate the minority class with synthetic samples. This approach capitalizes on the notion of “Cluster Cores,” representing locally dense regions within clusters. These Cluster Cores act as central, densely crowded areas that capture intricate topological properties of the corresponding clusters, especially in complex datasets with a non-convex spatial orientation in the feature space. By concentrating on these high-density regions, our clustering-based oversampling technique generates synthetic samples within the convex hull region of minority class instances in the formed clusters. This strategy ensures the creation of points that align with the data space and considers each minority instance within a specific cluster, thereby averting the problems encountered due to the generation of artificial samples by mere linear combination of the minority class data points, as is encountered in SMOTE (Synthetic Minority Oversampling Technique)-based algorithms. To assess the efficacy of our proposal, we conducted experimental comparisons against several cutting-edge algorithms, considering an array of evaluation metrics on well-known datasets used in the literature for both binary and multi-class classification. Additionally, we undertook a detailed ablation study, scrutinized existing algorithms in our context, delineated their strengths and limitations, and contemplated potential research directions in this domain.},
  keywords={Clustering algorithms;Noise measurement;Interpolation;Noise;Computational intelligence;Classification algorithms;Task analysis;Classification;imbalanced data;oversampling;synthetic minority oversampling technique},
  doi={10.1109/TETCI.2024.3407784},
  ISSN={2471-285X},
  month={April},
  jctype={journal},
}


@inproceedings{ansari2024Mo2E,
  author={Ansari, Faizanuddin and Bhattacharya, Agnish and Saha, Biswajit and Das, Swagatam},
  booktitle={2024 IEEE International Symposium on Biomedical Imaging (ISBI)}, 
  title={Mo2E: Mixture of Two Experts for Class-Imbalanced Learning from Medical Images}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Class imbalance in the medical image dataset is almost inherent due to the limited availability of clinical data for certain diseases and patient populations. Under-represented classes in the training set affect the classification task because the classifier tends to learn more from the majority classes, which are more common in the dataset and ignore data from the minority classes. To mitigate this issue, we propose a method to learn using two different convolutional neural network-based experts; such experts try to learn boundaries within the head classes, between the head and tail classes, and within the tail classes. During expert training, we integrate the MixUp regularization method to augment imbalanced data, employing distinct data sampling strategies for more effective mixing compared to random selection in traditional MixUp. During the inference phase, we combine the logits of the different experts based on their expertise in the corresponding classes. This way, we can improve the accuracy of the head and tail classes. Experiments using highly imbalanced and long-tailed datasets demonstrate the effectiveness of the suggested framework.},
  keywords={Training;Head;Accuracy;Sociology;Tail;Inference algorithms;Task analysis;Imbalanced classification;Augmentation Technique;Resampling},
  doi={10.1109/ISBI56570.2024.10635212},
  ISSN={1945-8452},
  month={May},
  jctype={conference},
}


@inproceedings{ojha2024affinity,
  title={Affinity-based Homophily: Can we measure homophily of a graph without using node labels?},
  author={Ojha, Indranil and Bose, Kushal and Das, Swagatam},
  booktitle={The Second Tiny Papers Track at ICLR 2024},
  year={2024},
  abstract={The homophily (heterophily) ratio in a graph represents the proportion of edges connecting nodes with similar (dissimilar) class labels. Existing methods for estimating the homophily ratio typically rely on knowing the class labels of each node in the graph. While several algorithms address both homophilic and heterophilic graphs, they necessitate prior knowledge of the homophily ratio to choose the appropriate one. To address this limitation, we propose a novel metric for measuring homophily ratio without information about node labels. In our approach, we define learnable affinity vectors for each node, characterizing the expected feature relationships with its neighbors. Our method, Affinity-based Homophily, derives the homophily ratio using these affinity vectors, eliminating the need for prior node label information. We conducted experiments on various benchmark homophilic and heterophilic graphs, demonstrating the commendable performance of our homophily measure.},
  keywords={Graphs, Convolution, Homophily, Heterophily, Affinity},
  jctype={conference},
  url={https://openreview.net/forum?id=IsdDOrAowN},
}

@inproceedings{dutta2024lost,
  title={Lost in Translation: GANs' Inability to Generate Simple Probability Distributions},
  author={Dutta, Debanjan and Chakrabarty, Anish and Das, Swagatam},
  booktitle={The Second Tiny Papers Track at ICLR 2024},
  year={2024},
  url={https://openreview.net/forum?id=MUOmyZMEd4},
  keywords={Generative Adversarial Networks, Generative Models, Statistical Simulation},
  abstract={Since its inception, Generative Adversarial Networks (GAN) have marked a triumph in generative modeling. Its impeccable capacity to mimic observations from unknown probability distributions has positioned it as a widely used simulation tool. In typical applications, GANs find themselves simulating data rich in semantic information such as images or text out of random noise. As such, it is reasonable to expect that large parametric models such as GANs must be able to estimate standard theoretical probability densities with ease. In this paper, based on a series of disillusioning experimental findings, we show that GANs often fail to induce the simplest of statistical transformations between distributions. For example, starting with a standard Gaussian noise, GANs with 2-deep generators are unable to perform a positional translation. Supporting theoretical tests on generated data further corroborates our rather unsettling conclusions.},
  jctype={conference},
}


